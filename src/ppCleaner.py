import os
from pathlib import Path

from bs4 import BeautifulSoup as bs
import pandas as pd

import src
from src import ppSplitter
from src import utils


def extract_article_data(html, newsroom_nr):
    """
    FUNCTION
    Extracts some specific informations from the html of an article page.
    It returns a dict containing the informations together with the newsroom_nr.

    INPUT
    html: beautiful soup html parser object
    newsroom_nr: number in the newsroom link / newsroom-id

    OUTPUT
    data: dict containing the extracted informations
    """

    # url
    article_link = html.find("meta", {"name": "og:url"})["content"]

    # institution / newsroom
    institution = html.find("a", "story-customer").text

    # datetime of publishing
    dt = html.find("p", class_="date").text
    dt = dt.split("â€“")

    date = dt[0].strip().split(".")
    year = date[2]
    month = date[1]
    day = date[0]

    time = dt[1].strip().split(":")
    hour = time[0]
    minute = time[1]

    date = "-".join([year, month, day])
    time = ":".join([hour, minute])
    dt = " ".join([date, time])

    # get the main location of event
    location = html.find("a", class_="story-city event-trigger").text

    # story header
    header = html.find("h1").text

    # story text (at this point the date, the institution etc. are still included)
    text = html.find("div", class_="card")
    text = text.find_all("p")
    text = [p.text for p in text]
    text = "\n\n".join(text)

    # get tags used by presseportal
    tags = html.find_all("ul", class_="tags")

    # tags for locations
    location_tags = tags[0]
    location_tags = location_tags.find_all("li")
    location_tags_names = [tag.find("a").text for tag in location_tags]
    location_tags_scores = [tag["data-score"] for tag in location_tags]

    # tags for topics in story
    topic_tags = tags[1]
    topic_tags = topic_tags.find_all("li")
    topic_tags_names = [tag.find("a").text for tag in topic_tags]
    topic_tags_scores = [tag["data-score"] for tag in topic_tags]

    data = {
        "article_link": article_link,
        "newsroom_nr": newsroom_nr,
        "from_presseportal": 1,
        "newsroom": institution,
        "date_release": dt,
        "location": location,
        "header": header,
        "text": text,
        "location_tags_names": location_tags_names,
        "location_tags_scores": location_tags_scores,
        "topic_tags_names": topic_tags_names,
        "topic_tags_scores": topic_tags_scores,
    }

    return data


def html_to_df(state, year, output_folder_name):
    utils.print_status("start converting raw html files to datasets.")

    year = str(year)

    state_newsroom_datasets = []

    output_folder_path = Path.joinpath(src.PATH, "output_data", output_folder_name)

    utils.create_folder(Path.joinpath(output_folder_path, "articles", "state_datasets"))

    state_year_data_folder_path = Path.joinpath(
        output_folder_path,
        "articles",
        "raw_article_html",
        state,
        year,
    )

    # list of subfolders for each newsroom
    newsroom_folder_names = os.listdir(state_year_data_folder_path)

    number_of_newsrooms = len(newsroom_folder_names)

    # for each newsroom
    for i, newsroom_folder_name in enumerate(newsroom_folder_names):
        # progress
        overall_progress = round(((i + 1) / number_of_newsrooms) * 100)
        utils.print_status(
            "start converting raw html files to datasets for newsroom "
            + str(newsroom_folder_name)
        )

        # build path to newsroom-folder
        newsroom_folder_path = Path.joinpath(
            state_year_data_folder_path, newsroom_folder_name
        )

        # list of all files in this newsroom-folder
        list_of_html_files = os.listdir(newsroom_folder_path)

        # list for storing the newsroom_data
        newsroom_dataset = []

        # for each file in newsroom-folder
        for j, html_file_name in enumerate(list_of_html_files):
            number_of_html_files = len(list_of_html_files)

            # build path to this file
            html_file_path = Path.joinpath(newsroom_folder_path, html_file_name)

            # if it is a txt-file
            if str(html_file_path).endswith(".txt"):
                # open, read, close
                file = open(html_file_path, "r", encoding="utf-8")
                html = file.read()
                file.close()

                # convert to a bs-object
                html = bs(html, "html.parser")

                try:
                    # extract informations from html
                    extracted_data = extract_article_data(html, newsroom_folder_name)

                    scraping_date_time = html_file_name.split("_")[-1].strip(".txt")
                    extracted_data.update({"scraping_date_time": scraping_date_time})

                    # append result (a dict) to list
                    newsroom_dataset.append(extracted_data)

                except Exception as error:
                    print(error, state, html_file_path)

        # convert result of the current newsroom to dataframe
        newsroom_dataset = pd.DataFrame(newsroom_dataset)
        newsroom_dataset["state"] = state

        state_newsroom_datasets.append(newsroom_dataset)

    state_year_df = pd.concat(state_newsroom_datasets)

    state_datasets_folder_path = Path.joinpath(
        output_folder_path,
        "articles",
        "state_datasets",
        state,
    )
    utils.create_folder(state_datasets_folder_path)

    state_year_dataset_file_name = state + "_" + year + ".csv"
    state_year_df.to_csv(
        Path.joinpath(state_datasets_folder_path, state_year_dataset_file_name),
        index=False,
    )

    utils.print_status("finished converting raw html files to datasets.")


def split_reports_bw(state, year, output_folder_name, n=False):
    """
    Wrapper function to split articles into reports and return them as csv file.
    
    Articles contain reports. But sometimes, one article contains several reports.  
    This function reads the articles from the csv files and uses the function
    `ppSplitter.split_reports_in_df` to split the articles when multiple reports 
    are present. 
    
    Returns:
        csv files of splitted reports per state and year 
    """
    year = str(year)

    # load data
    folder_path = Path.joinpath(
        src.PATH,
        "output_data",
        output_folder_name,
        "articles",
        "state_datasets",
        state,
    )
    df = pd.read_csv(Path.joinpath(folder_path, state + "_" + year + ".csv"))

    # draw a sample?
    if n:
        df = df.sample(n=n)

    # split texts
    df_split = ppSplitter.split_reports_in_df(df, "text")

    # save
    df_split.to_csv(
        Path.joinpath(folder_path, state + "_" + year + "_split.csv"), index=False
    )


def splitted_reports_csv_to_db(output_folder_name, state, year, engine):
    """
    Imports csv files of splitted reports and writes reports to database
    """
    year = str(year)
    
    # load data
    folder_path = Path.joinpath(
        src.PATH,
        "output_data",
        output_folder_name,
        "articles",
        "state_datasets",
        state,
    )

    df = pd.read_csv(Path.joinpath(folder_path, state + "_" + year + "_split.csv"))
    try:
        df = df.drop(columns=["Unnamed: 0", "Unnamed: 0.1"])
    except:
        pass
    df.to_sql(name="reports", con=engine, if_exists="append", index=False)
